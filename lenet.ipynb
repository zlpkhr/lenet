{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet-5 Fault Injection\n",
    "\n",
    "PyTorch requires defining forward to specify data flow through network layers - it's called automatically when you run lenet(input). The code inherits from nn.Module for automatic gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Lenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.s2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.c3 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.s4 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.f5 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        self.f7 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.c1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.s2(x)\n",
    "        x = self.c3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.s4(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.f5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.f6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.f7(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "lenet = Lenet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up data loading for the MNIST dataset. `NUM_WORKERS` is set to optimize CPU usage (max 4 threads) and batch size is 64. The transforms prep images by:\n",
    "- Converting to tensors\n",
    "- Normalizing with MNIST's mean (0.1307) and std (0.3081) \n",
    "- Adding 2-pixel padding to match LeNet input size\n",
    "\n",
    "The MNIST dataset is downloaded and split:\n",
    "- Test set (standard split)\n",
    "- Training set split into 80% train, 20% validation\n",
    "\n",
    "`DataLoader` wraps these datasets to enable batch processing with specified workers. Training data is shuffled, validation/test aren't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "NUM_WORKERS = min(4, os.cpu_count())\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.1307,), (0.3081,)),\n",
    "        T.Pad(2),\n",
    "    ]\n",
    ")\n",
    "train_set = datasets.MNIST(\"tmp/data\", train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(\"tmp/data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(train_set))\n",
    "val_size = len(train_set) - train_size\n",
    "\n",
    "train_set, val_set = random_split(train_set, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_set, BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "exp_loader = DataLoader(test_set, BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function evaluates model accuracy by running inference in evaluation mode (`model.eval()`), disabling gradients for efficiency, and calculating the percentage of correct predictions by comparing model outputs against target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model: nn.Module, loader: DataLoader) -> float:\n",
    "    model.eval()\n",
    "    total = correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input, target in loader:\n",
    "            output = model(input)\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            total += target.size(0)\n",
    "            correct += (pred == target).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop running for 3 epochs using Adam optimizer with learning rate 0.01. Each iteration zeros gradients, runs forward pass through LeNet, calculates cross-entropy loss between predictions and targets, backpropagates gradients (loss.backward()), and updates model weights (optimizer.step()). Progress prints every 100 batches. Finally tests model accuracy on test set using previously defined calculate_accuracy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINING] epoch=1 batch=0 loss=2.31\n",
      "[TRAINING] epoch=1 batch=100 loss=0.27\n",
      "[TRAINING] epoch=1 batch=200 loss=0.10\n",
      "[TRAINING] epoch=1 batch=300 loss=0.28\n",
      "[TRAINING] epoch=1 batch=400 loss=0.19\n",
      "[TRAINING] epoch=1 batch=500 loss=0.11\n",
      "[TRAINING] epoch=1 batch=600 loss=0.01\n",
      "[TRAINING] epoch=1 batch=700 loss=0.06\n",
      "[TRAINING] epoch=2 batch=0 loss=0.11\n",
      "[TRAINING] epoch=2 batch=100 loss=0.09\n",
      "[TRAINING] epoch=2 batch=200 loss=0.19\n",
      "[TRAINING] epoch=2 batch=300 loss=0.07\n",
      "[TRAINING] epoch=2 batch=400 loss=0.31\n",
      "[TRAINING] epoch=2 batch=500 loss=0.19\n",
      "[TRAINING] epoch=2 batch=600 loss=0.02\n",
      "[TRAINING] epoch=2 batch=700 loss=0.04\n",
      "[TRAINING] epoch=3 batch=0 loss=0.16\n",
      "[TRAINING] epoch=3 batch=100 loss=0.06\n",
      "[TRAINING] epoch=3 batch=200 loss=0.12\n",
      "[TRAINING] epoch=3 batch=300 loss=0.01\n",
      "[TRAINING] epoch=3 batch=400 loss=0.10\n",
      "[TRAINING] epoch=3 batch=500 loss=0.20\n",
      "[TRAINING] epoch=3 batch=600 loss=0.06\n",
      "[TRAINING] epoch=3 batch=700 loss=0.06\n",
      "[TESTING] accuracy=96.54%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "optimizer = optim.Adam(lenet.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "lenet.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (input, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        flip_output = lenet(input)\n",
    "        loss = F.cross_entropy(flip_output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"[TRAINING] epoch={epoch + 1} batch={batch_idx} loss={loss.item():.2f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "print(f\"[TESTING] accuracy={(100 * calculate_accuracy(lenet, exp_loader)):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves trained LeNet model's parameters (weights and biases) to a file named \"lenet.pt\" using PyTorch's save function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENET_PATH = \"lenet.pt\"\n",
    "\n",
    "torch.save(lenet.state_dict(), LENET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a quantization-ready version of LeNet by subclassing it. QLenet adds quantization (QuantStub) and dequantization (DeQuantStub) layers around the original network's forward pass. These stubs mark where PyTorch should convert between floating-point and quantized (8-bit integer) representations. The model loads weights from the previously saved LeNet using load_state_dict, with weights_only=True to ignore any saved optimizer state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "\n",
    "class QLenet(Lenet):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.quant(x)\n",
    "        x = super().forward(x)\n",
    "        x = self.dequant(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "qlenet = QLenet()\n",
    "\n",
    "qlenet.load_state_dict(torch.load(LENET_PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code configures quantization using Facebook's FBGEMM backend, prepares the model for quantization, then calibrates it by running validation data through the network to collect statistics about activation ranges. Finally, it converts the model to use quantized (8-bit) weights and tests its accuracy. The `inplace=True` parameter modifies the model directly instead of creating copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/lenet/.venv/lib/python3.11/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CALIBRATING] batch=0\n",
      "[CALIBRATING] batch=1\n",
      "[CALIBRATING] batch=2\n",
      "[CALIBRATING] batch=3\n",
      "[CALIBRATING] batch=4\n",
      "[CALIBRATING] batch=5\n",
      "[CALIBRATING] batch=6\n",
      "[CALIBRATING] batch=7\n",
      "[CALIBRATING] batch=8\n",
      "[CALIBRATING] batch=9\n",
      "[CALIBRATING] batch=10\n",
      "[CALIBRATING] batch=11\n",
      "[CALIBRATING] batch=12\n",
      "[CALIBRATING] batch=13\n",
      "[CALIBRATING] batch=14\n",
      "[CALIBRATING] batch=15\n",
      "[CALIBRATING] batch=16\n",
      "[CALIBRATING] batch=17\n",
      "[CALIBRATING] batch=18\n",
      "[CALIBRATING] batch=19\n",
      "[CALIBRATING] batch=20\n",
      "[CALIBRATING] batch=21\n",
      "[CALIBRATING] batch=22\n",
      "[CALIBRATING] batch=23\n",
      "[CALIBRATING] batch=24\n",
      "[CALIBRATING] batch=25\n",
      "[CALIBRATING] batch=26\n",
      "[CALIBRATING] batch=27\n",
      "[CALIBRATING] batch=28\n",
      "[CALIBRATING] batch=29\n",
      "[CALIBRATING] batch=30\n",
      "[CALIBRATING] batch=31\n",
      "[CALIBRATING] batch=32\n",
      "[CALIBRATING] batch=33\n",
      "[CALIBRATING] batch=34\n",
      "[CALIBRATING] batch=35\n",
      "[CALIBRATING] batch=36\n",
      "[CALIBRATING] batch=37\n",
      "[CALIBRATING] batch=38\n",
      "[CALIBRATING] batch=39\n",
      "[CALIBRATING] batch=40\n",
      "[CALIBRATING] batch=41\n",
      "[CALIBRATING] batch=42\n",
      "[CALIBRATING] batch=43\n",
      "[CALIBRATING] batch=44\n",
      "[CALIBRATING] batch=45\n",
      "[CALIBRATING] batch=46\n",
      "[CALIBRATING] batch=47\n",
      "[CALIBRATING] batch=48\n",
      "[CALIBRATING] batch=49\n",
      "[CALIBRATING] batch=50\n",
      "[CALIBRATING] batch=51\n",
      "[CALIBRATING] batch=52\n",
      "[CALIBRATING] batch=53\n",
      "[CALIBRATING] batch=54\n",
      "[CALIBRATING] batch=55\n",
      "[CALIBRATING] batch=56\n",
      "[CALIBRATING] batch=57\n",
      "[CALIBRATING] batch=58\n",
      "[CALIBRATING] batch=59\n",
      "[CALIBRATING] batch=60\n",
      "[CALIBRATING] batch=61\n",
      "[CALIBRATING] batch=62\n",
      "[CALIBRATING] batch=63\n",
      "[CALIBRATING] batch=64\n",
      "[CALIBRATING] batch=65\n",
      "[CALIBRATING] batch=66\n",
      "[CALIBRATING] batch=67\n",
      "[CALIBRATING] batch=68\n",
      "[CALIBRATING] batch=69\n",
      "[CALIBRATING] batch=70\n",
      "[CALIBRATING] batch=71\n",
      "[CALIBRATING] batch=72\n",
      "[CALIBRATING] batch=73\n",
      "[CALIBRATING] batch=74\n",
      "[CALIBRATING] batch=75\n",
      "[CALIBRATING] batch=76\n",
      "[CALIBRATING] batch=77\n",
      "[CALIBRATING] batch=78\n",
      "[CALIBRATING] batch=79\n",
      "[CALIBRATING] batch=80\n",
      "[CALIBRATING] batch=81\n",
      "[CALIBRATING] batch=82\n",
      "[CALIBRATING] batch=83\n",
      "[CALIBRATING] batch=84\n",
      "[CALIBRATING] batch=85\n",
      "[CALIBRATING] batch=86\n",
      "[CALIBRATING] batch=87\n",
      "[CALIBRATING] batch=88\n",
      "[CALIBRATING] batch=89\n",
      "[CALIBRATING] batch=90\n",
      "[CALIBRATING] batch=91\n",
      "[CALIBRATING] batch=92\n",
      "[CALIBRATING] batch=93\n",
      "[CALIBRATING] batch=94\n",
      "[CALIBRATING] batch=95\n",
      "[CALIBRATING] batch=96\n",
      "[CALIBRATING] batch=97\n",
      "[CALIBRATING] batch=98\n",
      "[CALIBRATING] batch=99\n",
      "[CALIBRATING] batch=100\n",
      "[CALIBRATING] batch=101\n",
      "[CALIBRATING] batch=102\n",
      "[CALIBRATING] batch=103\n",
      "[CALIBRATING] batch=104\n",
      "[CALIBRATING] batch=105\n",
      "[CALIBRATING] batch=106\n",
      "[CALIBRATING] batch=107\n",
      "[CALIBRATING] batch=108\n",
      "[CALIBRATING] batch=109\n",
      "[CALIBRATING] batch=110\n",
      "[CALIBRATING] batch=111\n",
      "[CALIBRATING] batch=112\n",
      "[CALIBRATING] batch=113\n",
      "[CALIBRATING] batch=114\n",
      "[CALIBRATING] batch=115\n",
      "[CALIBRATING] batch=116\n",
      "[CALIBRATING] batch=117\n",
      "[CALIBRATING] batch=118\n",
      "[CALIBRATING] batch=119\n",
      "[CALIBRATING] batch=120\n",
      "[CALIBRATING] batch=121\n",
      "[CALIBRATING] batch=122\n",
      "[CALIBRATING] batch=123\n",
      "[CALIBRATING] batch=124\n",
      "[CALIBRATING] batch=125\n",
      "[CALIBRATING] batch=126\n",
      "[CALIBRATING] batch=127\n",
      "[CALIBRATING] batch=128\n",
      "[CALIBRATING] batch=129\n",
      "[CALIBRATING] batch=130\n",
      "[CALIBRATING] batch=131\n",
      "[CALIBRATING] batch=132\n",
      "[CALIBRATING] batch=133\n",
      "[CALIBRATING] batch=134\n",
      "[CALIBRATING] batch=135\n",
      "[CALIBRATING] batch=136\n",
      "[CALIBRATING] batch=137\n",
      "[CALIBRATING] batch=138\n",
      "[CALIBRATING] batch=139\n",
      "[CALIBRATING] batch=140\n",
      "[CALIBRATING] batch=141\n",
      "[CALIBRATING] batch=142\n",
      "[CALIBRATING] batch=143\n",
      "[CALIBRATING] batch=144\n",
      "[CALIBRATING] batch=145\n",
      "[CALIBRATING] batch=146\n",
      "[CALIBRATING] batch=147\n",
      "[CALIBRATING] batch=148\n",
      "[CALIBRATING] batch=149\n",
      "[CALIBRATING] batch=150\n",
      "[CALIBRATING] batch=151\n",
      "[CALIBRATING] batch=152\n",
      "[CALIBRATING] batch=153\n",
      "[CALIBRATING] batch=154\n",
      "[CALIBRATING] batch=155\n",
      "[CALIBRATING] batch=156\n",
      "[CALIBRATING] batch=157\n",
      "[CALIBRATING] batch=158\n",
      "[CALIBRATING] batch=159\n",
      "[CALIBRATING] batch=160\n",
      "[CALIBRATING] batch=161\n",
      "[CALIBRATING] batch=162\n",
      "[CALIBRATING] batch=163\n",
      "[CALIBRATING] batch=164\n",
      "[CALIBRATING] batch=165\n",
      "[CALIBRATING] batch=166\n",
      "[CALIBRATING] batch=167\n",
      "[CALIBRATING] batch=168\n",
      "[CALIBRATING] batch=169\n",
      "[CALIBRATING] batch=170\n",
      "[CALIBRATING] batch=171\n",
      "[CALIBRATING] batch=172\n",
      "[CALIBRATING] batch=173\n",
      "[CALIBRATING] batch=174\n",
      "[CALIBRATING] batch=175\n",
      "[CALIBRATING] batch=176\n",
      "[CALIBRATING] batch=177\n",
      "[CALIBRATING] batch=178\n",
      "[CALIBRATING] batch=179\n",
      "[CALIBRATING] batch=180\n",
      "[CALIBRATING] batch=181\n",
      "[CALIBRATING] batch=182\n",
      "[CALIBRATING] batch=183\n",
      "[CALIBRATING] batch=184\n",
      "[CALIBRATING] batch=185\n",
      "[CALIBRATING] batch=186\n",
      "[CALIBRATING] batch=187\n",
      "[TESTING] accuracy=96.27%\n"
     ]
    }
   ],
   "source": [
    "import torch.quantization as quantization\n",
    "\n",
    "BACKEND = \"fbgemm\"\n",
    "\n",
    "qlenet.qconfig = quantization.get_default_qconfig(BACKEND)\n",
    "quantization.prepare(qlenet, inplace=True)\n",
    "\n",
    "qlenet.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (input, _) in enumerate(val_loader):\n",
    "        qlenet(input)\n",
    "        print(f\"[CALIBRATING] batch={batch_idx}\")\n",
    "\n",
    "quantization.convert(qlenet, inplace=True)\n",
    "\n",
    "quant_accuracy = 100 * calculate_accuracy(qlenet, exp_loader)\n",
    "print(f\"[TESTING] accuracy={quant_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the quantized LeNet model's state dictionary to \"qlenet.pt\" file, preserving the quantized weights and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QLENET_PATH = \"qlenet.pt\"\n",
    "\n",
    "torch.save(qlenet.state_dict(), QLENET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code recreates a new quantization-ready LeNet (flenet), prepares and converts it for quantization without calibration, then loads the previously saved quantized weights from \"qlenet.pt\". This way the model starts directly with quantized weights instead of needing recalibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/lenet/.venv/lib/python3.11/site-packages/torch/ao/quantization/observer.py:1315: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "/root/lenet/.venv/lib/python3.11/site-packages/torch/_utils.py:413: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flenet = QLenet()\n",
    "\n",
    "flenet.qconfig = quantization.get_default_qconfig(BACKEND)\n",
    "quantization.prepare(flenet, inplace=True)\n",
    "quantization.convert(flenet, inplace=True)\n",
    "\n",
    "flenet.load_state_dict(torch.load(QLENET_PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are utility functions for manipulating quantized model weights, including bit flipping, layer selection, tensor conversion, and quantization operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple\n",
    "import numpy as np\n",
    "from torch import quantize_per_tensor\n",
    "import random\n",
    "\n",
    "def get_weight_key(model: nn.Module, attr: str) -> str:\n",
    "    \"\"\"Retrieves the appropriate weight key for accessing model parameters based on layer type.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network module\n",
    "        attr: Layer attribute name\n",
    "        \n",
    "    Returns:\n",
    "        String key to access weights in state dict\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If layer type is not supported (not Conv2d or Linear)\n",
    "    \"\"\"\n",
    "    module = getattr(model, attr)\n",
    "    if isinstance(module, torch.ao.nn.quantized.modules.conv.Conv2d):\n",
    "        return f\"{attr}.weight\"\n",
    "    elif isinstance(module, torch.ao.nn.quantized.modules.linear.Linear):\n",
    "        return f\"{attr}._packed_params._packed_params\"\n",
    "    raise ValueError(f\"Unsupported module type: {type(module)}\")\n",
    "\n",
    "def get_bit(byte: np.int8, pos: int) -> np.int8:\n",
    "    \"\"\"Extracts the bit at specified position from an 8-bit integer.\n",
    "    \n",
    "    Args:\n",
    "        byte: 8-bit integer input\n",
    "        pos: Bit position to extract (0-7)\n",
    "    \n",
    "    Returns:\n",
    "        Value of bit at specified position (0 or 1)\n",
    "    \"\"\"\n",
    "    return np.bitwise_and(np.right_shift(byte, pos), np.int8(1))\n",
    "\n",
    "def flip_bit(byte: np.int8, pos: int) -> np.int8:\n",
    "    \"\"\"Flips (inverts) the bit at specified position in an 8-bit integer.\n",
    "    \n",
    "    Args:\n",
    "        byte: 8-bit integer input\n",
    "        pos: Position of bit to flip (0-7)\n",
    "    \n",
    "    Returns:\n",
    "        New 8-bit integer with specified bit flipped\n",
    "    \"\"\"\n",
    "    return np.bitwise_xor(byte, np.left_shift(np.int8(1), pos))\n",
    "\n",
    "# Tuple of suffixes used to identify weight parameters in state dict\n",
    "WEIGHT_SUFFIXES = (\".weight\", \"._packed_params._packed_params\")\n",
    "\n",
    "def randlayer(sd: Dict[str, Any]) -> torch.Tensor:\n",
    "    \"\"\"Randomly selects a layer from model state dictionary.\n",
    "    \n",
    "    Args:\n",
    "        sd: Model state dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Key of randomly selected layer\n",
    "    \"\"\"\n",
    "    layers = [k for k in sd.keys() if k.endswith(WEIGHT_SUFFIXES)]\n",
    "    return random.choice(layers)\n",
    "\n",
    "def clean_layer(layer: str) -> str:\n",
    "    \"\"\"Removes weight-related suffixes from layer name.\n",
    "    \n",
    "    Args:\n",
    "        layer: Full layer name with suffix\n",
    "        \n",
    "    Returns:\n",
    "        Clean layer name without suffix\n",
    "    \"\"\"\n",
    "    for suffix in WEIGHT_SUFFIXES:\n",
    "        layer = layer.removesuffix(suffix)\n",
    "    return layer\n",
    "\n",
    "def to_tensor(param) -> torch.Tensor:\n",
    "    \"\"\"Converts parameter to tensor if not already.\n",
    "    \n",
    "    Args:\n",
    "        param: Model parameter (tensor or tuple)\n",
    "        \n",
    "    Returns:\n",
    "        Parameter as tensor\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If parameter type is not supported\n",
    "    \"\"\"\n",
    "    if isinstance(param, torch.Tensor):\n",
    "        return param\n",
    "    elif isinstance(param, tuple) and len(param) > 0:\n",
    "        return param[0]\n",
    "    raise ValueError(f\"Unsupported parameter type: {type(param)}\")\n",
    "\n",
    "def randidx(tensor: torch.Tensor) -> Tuple[int, ...]:\n",
    "    \"\"\"Generates random indices for each dimension of tensor.\n",
    "    \n",
    "    Args:\n",
    "        tensor: Input tensor\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of random indices matching tensor dimensions\n",
    "    \"\"\"\n",
    "    return tuple(random.randint(0, s - 1) for s in tensor.shape)\n",
    "\n",
    "def randbitpos() -> int:\n",
    "    \"\"\"Generates random bit position (0-7) for 8-bit integer.\n",
    "    \n",
    "    Returns:\n",
    "        Random integer between 0 and 7\n",
    "    \"\"\"\n",
    "    return random.randint(0, 7)\n",
    "\n",
    "def dequant_int8(val: np.int8, scale: float, zero_point: int) -> float:\n",
    "    \"\"\"Dequantizes 8-bit integer to floating point value.\n",
    "    \n",
    "    Args:\n",
    "        val: Quantized 8-bit value\n",
    "        scale: Quantization scale factor\n",
    "        zero_point: Quantization zero point\n",
    "        \n",
    "    Returns:\n",
    "        Dequantized floating point value\n",
    "    \"\"\"\n",
    "    return float(val - zero_point) * scale\n",
    "\n",
    "def quant_to_int8(tensor: torch.Tensor) -> np.int8:\n",
    "    \"\"\"Converts quantized tensor to 8-bit integer representation.\n",
    "    \n",
    "    Args:\n",
    "        tensor: Quantized input tensor\n",
    "        \n",
    "    Returns:\n",
    "        NumPy array of 8-bit integers\n",
    "    \"\"\"\n",
    "    return tensor.int_repr().numpy().astype(np.int8)\n",
    "\n",
    "def int8_to_quant(input: np.int8, scale: float, zero_point: int) -> torch.Tensor:\n",
    "    \"\"\"Converts 8-bit integer to quantized tensor.\n",
    "    \n",
    "    Args:\n",
    "        input: 8-bit integer input\n",
    "        scale: Quantization scale factor\n",
    "        zero_point: Quantization zero point\n",
    "        \n",
    "    Returns:\n",
    "        Quantized tensor\n",
    "    \"\"\"\n",
    "    return quantize_per_tensor(\n",
    "        torch.tensor(dequant_int8(input, scale, zero_point), dtype=torch.float32),\n",
    "        scale,\n",
    "        zero_point,\n",
    "        torch.qint8,\n",
    "    )\n",
    "\n",
    "def infer(input: torch.Tensor, model: nn.Module) -> torch.Tensor:\n",
    "    \"\"\"Performs inference using quantized model.\n",
    "    \n",
    "    Args:\n",
    "        input: Input tensor\n",
    "        model: Neural network model\n",
    "        \n",
    "    Returns:\n",
    "        Model prediction (argmax of output)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return torch.argmax(model(input), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a DataLoader for test dataset with specified batch size and number of worker threads for parallel data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_loader = DataLoader(test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs fault injection testing on a quantized neural network by iterating through test data, randomly selecting and flipping bits in model parameters, and recording the impact on model outputs. For each input image, it creates a copy of the model's state, randomly selects a layer and parameter, flips a random bit in that parameter, runs inference with the modified model, and stores the results (including layer information, bit positions, original and flipped parameter values, and model outputs) in a DataFrame that's ultimately saved to a CSV file. After each test, it restores the model to its original state before proceeding to the next input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "results_rand = []\n",
    "orig_sd = flenet.state_dict()\n",
    "\n",
    "for inputs, targets in exp_loader:\n",
    "    for input, target in zip(inputs, targets):\n",
    "        input = input.unsqueeze(0)\n",
    "        output = infer(input, flenet)\n",
    "\n",
    "        sd = copy.deepcopy(orig_sd)\n",
    "\n",
    "        layer = randlayer(sd)\n",
    "        params = to_tensor(sd[layer])\n",
    "        idx = randidx(params)\n",
    "        param = params[idx]\n",
    "\n",
    "        orig_param = quant_to_int8(param)\n",
    "\n",
    "        bit_pos = randbitpos()\n",
    "        orig_bit = get_bit(orig_param, bit_pos)\n",
    "        flipped_bit = 1 if orig_bit == 0 else 0\n",
    "\n",
    "        flipped_param = flip_bit(orig_param, bit_pos)\n",
    "\n",
    "        params[idx] = int8_to_quant(\n",
    "            flipped_param, param.q_scale(), param.q_zero_point()\n",
    "        )\n",
    "\n",
    "        flenet.load_state_dict(sd)\n",
    "\n",
    "        flip_output = infer(input, flenet)\n",
    "\n",
    "        results_rand.append(\n",
    "            {\n",
    "                \"layer\": clean_layer(layer),\n",
    "                \"bit_position\": bit_pos,\n",
    "                \"original_bit\": orig_bit,\n",
    "                \"flipped_bit\": flipped_bit,\n",
    "                \"original_param\": orig_param,\n",
    "                \"flipped_param\": flipped_param,\n",
    "                \"original_output\": output.item(),\n",
    "                \"flipped_output\": flip_output.item(),\n",
    "                \"target_output\": target.item(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        flenet.load_state_dict(orig_sd)\n",
    "\n",
    "df_rand = pd.DataFrame(results_rand)\n",
    "df_rand.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>bit_position</th>\n",
       "      <th>original_bit</th>\n",
       "      <th>flipped_bit</th>\n",
       "      <th>original_param</th>\n",
       "      <th>flipped_param</th>\n",
       "      <th>original_output</th>\n",
       "      <th>flipped_output</th>\n",
       "      <th>target_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>c1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>c1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-37</td>\n",
       "      <td>-53</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>c1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-79</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>c1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-65</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>f7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>119</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>c1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>-103</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>c1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-29</td>\n",
       "      <td>99</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4424</th>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-37</td>\n",
       "      <td>-5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4528</th>\n",
       "      <td>f7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "      <td>124</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4709</th>\n",
       "      <td>f7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4944</th>\n",
       "      <td>c1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5771</th>\n",
       "      <td>c1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174</th>\n",
       "      <td>c1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>-98</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8320</th>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "      <td>-37</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9236</th>\n",
       "      <td>f7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-8</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9839</th>\n",
       "      <td>c1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer  bit_position  original_bit  flipped_bit original_param  \\\n",
       "447     c1             0             0            1             46   \n",
       "557     c1             4             1            0            -37   \n",
       "1062    c1             7             1            0            -79   \n",
       "1811    c1             7             1            0            -65   \n",
       "2628    f7             7             1            0             -9   \n",
       "2939    c1             7             0            1             25   \n",
       "3273    c1             7             1            0            -29   \n",
       "4424    c1             5             0            1            -37   \n",
       "4528    f7             7             1            0             -4   \n",
       "4709    f7             4             0            1             36   \n",
       "4944    c1             4             1            0             30   \n",
       "5771    c1             6             1            0             75   \n",
       "7174    c1             7             0            1             30   \n",
       "8320    c1             5             1            0             -5   \n",
       "9236    f7             7             1            0             -8   \n",
       "9839    c1             6             0            1             32   \n",
       "\n",
       "      flipped_param  original_output  flipped_output  target_output  \n",
       "447              47                1               4              4  \n",
       "557             -53                7               2              7  \n",
       "1062             49                3               8              3  \n",
       "1811             63                2               3              2  \n",
       "2628            119                7               5              7  \n",
       "2939           -103                7               9              9  \n",
       "3273             99                6               5              6  \n",
       "4424             -5                4               9              9  \n",
       "4528            124                3               2              3  \n",
       "4709             52                1               2              2  \n",
       "4944             14                3               2              2  \n",
       "5771             11                8               5              8  \n",
       "7174            -98                9               4              9  \n",
       "8320            -37                7               2              2  \n",
       "9236            120                0               6              0  \n",
       "9839             96                2               7              2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_errors_df = df_rand[df_rand[\"original_output\"] != df_rand[\"flipped_output\"]]\n",
    "\n",
    "rand_errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "results_c1_5 = []\n",
    "orig_sd = flenet.state_dict()\n",
    "\n",
    "for inputs, targets in exp_loader:\n",
    "    for input, target in zip(inputs, targets):\n",
    "        input = input.unsqueeze(0)\n",
    "        output = infer(input, flenet)\n",
    "\n",
    "        sd = copy.deepcopy(orig_sd)\n",
    "\n",
    "        layer = get_weight_key(flenet, \"c1\")\n",
    "        params = to_tensor(sd[layer])\n",
    "        idx = randidx(params)\n",
    "        param = params[idx]\n",
    "\n",
    "        orig_param = quant_to_int8(param)\n",
    "\n",
    "        bit_pos = 5\n",
    "        orig_bit = get_bit(orig_param, bit_pos)\n",
    "        flipped_bit = 1 if orig_bit == 0 else 0\n",
    "\n",
    "        flipped_param = flip_bit(orig_param, bit_pos)\n",
    "\n",
    "        params[idx] = int8_to_quant(\n",
    "            flipped_param, param.q_scale(), param.q_zero_point()\n",
    "        )\n",
    "\n",
    "        flenet.load_state_dict(sd)\n",
    "\n",
    "        flip_output = infer(input, flenet)\n",
    "\n",
    "        results_c1_5.append(\n",
    "            {\n",
    "                \"layer\": clean_layer(layer),\n",
    "                \"bit_position\": bit_pos,\n",
    "                \"original_bit\": orig_bit,\n",
    "                \"flipped_bit\": flipped_bit,\n",
    "                \"original_param\": orig_param,\n",
    "                \"flipped_param\": flipped_param,\n",
    "                \"original_output\": output.item(),\n",
    "                \"flipped_output\": flip_output.item(),\n",
    "                \"target_output\": target.item(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        flenet.load_state_dict(orig_sd)\n",
    "\n",
    "df_c1_5 = pd.DataFrame(results_c1_5)\n",
    "df_c1_5.to_csv(\"results_c1_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>bit_position</th>\n",
       "      <th>original_bit</th>\n",
       "      <th>flipped_bit</th>\n",
       "      <th>original_param</th>\n",
       "      <th>flipped_param</th>\n",
       "      <th>original_output</th>\n",
       "      <th>flipped_output</th>\n",
       "      <th>target_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-11</td>\n",
       "      <td>-43</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-11</td>\n",
       "      <td>-43</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-67</td>\n",
       "      <td>-99</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-127</td>\n",
       "      <td>-95</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9161</th>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-21</td>\n",
       "      <td>-53</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9627</th>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-37</td>\n",
       "      <td>-5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9669</th>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-53</td>\n",
       "      <td>-21</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9679</th>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-20</td>\n",
       "      <td>-52</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9904</th>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     layer  bit_position  original_bit  flipped_bit original_param  \\\n",
       "1       c1             5             1            0            -11   \n",
       "125     c1             5             1            0            -11   \n",
       "193     c1             5             1            0            -67   \n",
       "247     c1             5             0            1           -127   \n",
       "447     c1             5             0            1             26   \n",
       "...    ...           ...           ...          ...            ...   \n",
       "9161    c1             5             1            0            -21   \n",
       "9627    c1             5             0            1            -37   \n",
       "9669    c1             5             0            1            -53   \n",
       "9679    c1             5             1            0            -20   \n",
       "9904    c1             5             0            1             64   \n",
       "\n",
       "      flipped_param  original_output  flipped_output  target_output  \n",
       "1               -43                1               2              2  \n",
       "125             -43                4               9              9  \n",
       "193             -99                8               9              9  \n",
       "247             -95                2               6              4  \n",
       "447              58                1               4              4  \n",
       "...             ...              ...             ...            ...  \n",
       "9161            -53                8               5              8  \n",
       "9627             -5                5               6              6  \n",
       "9669            -21                7               1              4  \n",
       "9679            -52                3               5              6  \n",
       "9904             96                2               0              2  \n",
       "\n",
       "[74 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1_5_errors_df = df_c1_5[df_c1_5[\"original_output\"] != df_c1_5[\"flipped_output\"]]\n",
    "\n",
    "c1_5_errors_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
